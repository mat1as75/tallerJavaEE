
# Integraci칩n de LLM para Clasificaci칩n de Reclamos

## 쯈u칠 es un LLM y para qu칠 se usa en este proyecto?

Un **LLM (Large Language Model)** es un modelo de lenguaje entrenado con grandes vol칰menes de texto que puede comprender, generar y clasificar lenguaje natural. En este proyecto se utiliz칩 un LLM local (LLaMA2) para **clasificar autom치ticamente mensajes de reclamo** como `POSITIVO`, `NEGATIVO` o `NEUTRO`, mejorando as칤 la capacidad de an치lisis del sistema de forma autom치tica y escalable.

## Objetivo de la Integraci칩n

El objetivo de esta funcionalidad fue automatizar la **etiquetaci칩n de quejas de clientes** usando procesamiento de lenguaje natural. De esta manera, el sistema puede tomar decisiones o priorizar reclamos seg칰n su tono, sin intervenci칩n humana.

## Instalaci칩n y Ejecuci칩n de Llama2 Localmente

A continuaci칩n se detallan los pasos seguidos para instalar y ejecutar el modelo LLaMA2 mediante **Ollama** en Linux.

### Requisitos

- Docker Engine instalado

### Pasos de instalaci칩n y ejecuci칩n

#### 1. Ejecutar contenedor de Ollama (motor)

```bash
sudo docker run -d -p 11434:11434 --name ollama ollama/ollama
```

> Ollama es un gestor de modelos de lenguaje. Expone una API REST para interactuar con modelos como LLaMA2.

#### 2. Descargar el modelo LLaMA2

```bash
sudo docker exec -it ollama ollama pull llama2
```

Esto descargar치 el modelo LLaMA2 necesario para realizar clasificaci칩n de texto.

#### 3. Ejecutar el modelo

```bash
sudo docker exec -it ollama ollama run llama2
```

Esto pondr치 a correr el modelo y lo expondr치 v칤a API en `http://localhost:11434`.

---

## Uso del LLM en el Proyecto

La l칩gica del uso del modelo se encuentra encapsulada en las siguientes clases:

### 游댳 `LLMClient.java`

Esta clase es responsable de realizar la consulta al LLM (v칤a HTTP) y procesar la respuesta para obtener la clasificaci칩n (`POS/NEG/NEU`).

```java
String prompt = "Dado el siguiente reclamo, respond칠 칰nicamente con una de estas 3 palabras: POSITIVO, NEGATIVO o NEUTRO. Reclamo: \"" + textoReclamo + "\"";
```

La respuesta se analiza para detectar cu치l de las 3 etiquetas devuelve el modelo.

### 游댳 `ComplaintMessageConsumer.java`

Este componente es un `MessageListener` que escucha mensajes de la queue `ComplaintQueue`. Cuando llega un nuevo `ComplaintMessage`, se invoca al LLM para clasificar el texto del reclamo:

```java
qualification = LLMClient.clasificarReclamo(complaintMessage.getMessage()).toUpperCase(Locale.ROOT);
```

Luego se persiste la queja junto con su calificaci칩n mediante `CommerceRepository`.

---

## Flujo de Trabajo de la Clasificaci칩n

1. El comercio emite una queja desde la UI o API.
2. Se env칤a un mensaje a la **Queue JMS `ComplaintQueue`** (asincr칩nicamente).
3. El **Message-Driven Bean** `ComplaintMessageConsumer` consume el mensaje.
4. Se consulta al LLM mediante `LLMClient`.
5. La queja es **guardada en la base de datos** junto con su calificaci칩n (`Qualification`).

---

## Ejemplo de Request y Respuesta del LLM

### Prompt Enviado:

```json
{
  "model": "llama2",
  "prompt": "Dado el siguiente reclamo, respond칠 칰nicamente con una de estas 3 palabras: POSITIVO, NEGATIVO o NEUTRO. Reclamo: \"El sistema estuvo ca칤do 2 horas.\"",
  "stream": false
}
```

### Respuesta Esperada:

```json
{
  "response": "NEGATIVO"
}
```

---

## Recomendaciones

- El modelo LLaMA2 debe estar ejecut치ndose localmente mediante Ollama para que la funcionalidad est칠 disponible.
- Para ambientes productivos, considerar:
    - Aumentar la tolerancia a fallos del LLM (timeout, retries).
    - Posible integraci칩n con modelos LLM en la nube si se requiere mayor disponibilidad o potencia.
- Se puede expandir la clasificaci칩n para detectar sentimientos m치s complejos en el futuro.

---

## Consideraciones Finales

La incorporaci칩n de un modelo LLM para analizar y clasificar reclamos agrega **valor autom치tico** al proceso de gesti칩n de clientes, permitiendo:
- Reacciones m치s inteligentes ante quejas.
- Priorizaci칩n por nivel de gravedad.
- M칠tricas m치s 칰tiles para an치lisis interno.

El enfoque es modular y desacoplado, por lo que puede adaptarse f치cilmente a otros modelos o APIs de lenguaje.